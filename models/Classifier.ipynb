{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":8356547,"datasetId":3934836,"databundleVersionId":8489445},{"sourceType":"datasetVersion","sourceId":2983853,"datasetId":1828813,"databundleVersionId":3031585},{"sourceType":"datasetVersion","sourceId":3036086,"datasetId":1859421,"databundleVersionId":3084156},{"sourceType":"datasetVersion","sourceId":11044,"datasetId":7812,"databundleVersionId":11044},{"sourceType":"datasetVersion","sourceId":974,"datasetId":478,"databundleVersionId":974}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-25T12:55:35.843500Z","iopub.execute_input":"2024-09-25T12:55:35.843962Z","iopub.status.idle":"2024-09-25T12:55:36.350014Z","shell.execute_reply.started":"2024-09-25T12:55:35.843904Z","shell.execute_reply":"2024-09-25T12:55:36.348631Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/housing-prices-dataset/Housing.csv\n/kaggle/input/mushroom-classification/mushrooms.csv\n/kaggle/input/heart-disease-dataset/heart.csv\n/kaggle/input/healthcare-dataset/healthcare_dataset.csv\n/kaggle/input/logistic-regression/Social_Network_Ads.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:55:36.352370Z","iopub.execute_input":"2024-09-25T12:55:36.353064Z","iopub.status.idle":"2024-09-25T12:55:37.264088Z","shell.execute_reply.started":"2024-09-25T12:55:36.353001Z","shell.execute_reply":"2024-09-25T12:55:37.262949Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"! pip install imbalanced-learn","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:55:37.266440Z","iopub.execute_input":"2024-09-25T12:55:37.267148Z","iopub.status.idle":"2024-09-25T12:55:51.377567Z","shell.execute_reply.started":"2024-09-25T12:55:37.267092Z","shell.execute_reply":"2024-09-25T12:55:51.375998Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.10/site-packages (0.12.3)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.14.1)\nRequirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (3.5.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# classifier\nimport shap\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nimport time\nimport xgboost as xgb\n\ndef check_class_imbalance(y, threshold=0.1):\n    # Count the occurrences of each class\n    class_counts = y.value_counts()\n    \n    # Get the majority and minority class counts\n    majority_class_count = class_counts.max()\n    minority_class_count = class_counts.min()\n\n    # Calculate the imbalance ratio\n    imbalance_ratio = minority_class_count / majority_class_count\n\n    print(f\"Class distribution:\\n{class_counts}\\n\")\n    print(f\"Imbalance ratio (minority/majority): {imbalance_ratio:.2f}\")\n\n    # Check if the imbalance ratio is below the threshold\n    if imbalance_ratio < threshold:\n        print(\"Class imbalance detected. SMOTE is recommended.\")\n        return True\n    else:\n        print(\"No significant class imbalance detected.\")\n        return False\n\n# Preprocessing function with SHAP and SMOTE/undersampling/oversampling\ndef preprocess_data_with_sampling(df, target_column, sampling_strategy='none', shap_threshold=0.01):\n    # Handle missing values\n    df = df.dropna()\n    df.drop_duplicates(inplace = True)\n    \n    # Encode categorical features\n    label_encoders = {}\n    for col in df.select_dtypes(include=['object']).columns:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n        label_encoders[col] = le\n        \n    # Split data into features and labels\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Apply sampling strategy\n    if sampling_strategy == 'smote' and check_class_imbalance(y_train):\n        smote = SMOTE(random_state=42)\n        X_train, y_train = smote.fit_resample(X_train, y_train)\n    elif sampling_strategy == 'undersample':\n        undersample = RandomUnderSampler(random_state=42)\n        X_train, y_train = undersample.fit_resample(X_train, y_train)\n    elif sampling_strategy == 'oversample':\n        oversample = RandomOverSampler(random_state=42)\n        X_train, y_train = oversample.fit_resample(X_train, y_train)\n    elif sampling_strategy == 'smoteenn':\n        smote_enn = SMOTEENN(random_state=42)\n        X_train, y_train = smote_enn.fit_resample(X_train, y_train)\n    elif sampling_strategy == 'smotetomek':\n        smote_tomek = SMOTETomek(random_state=42)\n        X_train, y_train = smote_tomek.fit_resample(X_train, y_train)\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    # Train a base model (Random Forest)\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n    \n    # Calculate SHAP values\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_train)\n    \n    # Calculate mean absolute SHAP values for feature importance\n    shap_importance = np.mean(np.abs(shap_values[1]), axis=0)\n    feature_importance = pd.DataFrame({'feature': df.drop(target_column, axis=1).columns, \n                                       'importance': shap_importance})\n    \n    # Remove features with importance less than the threshold\n    important_features = feature_importance[feature_importance['importance'] > shap_threshold]['feature'].values\n    X_train = pd.DataFrame(X_train, columns=df.drop(target_column, axis=1).columns)[important_features]\n    X_test = pd.DataFrame(X_test, columns=df.drop(target_column, axis=1).columns)[important_features]\n    \n    return X_train, X_test, y_train, y_test\n\n# Function to evaluate models\ndef evaluate_model(X_train, X_test, y_train, y_test):\n    models = {\n        \"Logistic Regression\": LogisticRegression(),\n        \"Decision Tree\": DecisionTreeClassifier(),\n        \"Random Forest\": RandomForestClassifier(),\n        \"SVM\": SVC(),\n        \"KNN\": KNeighborsClassifier(),\n        \"Gradient Boosting\": GradientBoostingClassifier(),\n        \"XGBoost\": xgb.XGBClassifier(),\n        \"AdaBoost\": AdaBoostClassifier(),\n        \"Naive Bayes\": GaussianNB(),\n        \"MLP Neural Network\": MLPClassifier()\n    }\n    \n    for name, model in models.items():\n        start_time = time.time()\n        print(f\"Training {name}...\")\n        model = model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        time_consumed = time.time() - start_time\n        print(f\"{name} Accuracy: {np.round(accuracy,2)} Time: {time_consumed}\")\n\n# Run all classifiers with sampling strategies\ndef run_all_classifiers(df, target_column, sampling_strategy='none'):\n    X_train, X_test, y_train, y_test = preprocess_data_with_sampling(df, target_column, sampling_strategy)\n    evaluate_model(X_train, X_test, y_train, y_test)\n\n# Example usage with a dataset\nif __name__ == \"__main__\":\n    # Load your dataset here\n    df = pd.read_csv(\"/kaggle/input/mushroom-classification/mushrooms.csv\")\n    target_column = 'class'  # Change this to your dataset's target column\n    # Handle missing values\n    df = df.dropna()\n    df.drop_duplicates(inplace = True)\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    check=check_class_imbalance(y_train)\n    if(check==1):\n        # Run all classifiers with different sampling strategies\n        print(\"Running with SMOTE...\")\n        run_all_classifiers(df, target_column, sampling_strategy='smote')\n\n        print(\"\\nRunning with Random Oversampling...\")\n        run_all_classifiers(df, target_column, sampling_strategy='oversample')\n\n        print(\"\\nRunning with Random Undersampling...\")\n        run_all_classifiers(df, target_column, sampling_strategy='undersample')\n    else:\n         run_all_classifiers(df, target_column)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:56:44.930227Z","iopub.execute_input":"2024-09-25T12:56:44.930677Z","iopub.status.idle":"2024-09-25T12:56:57.850936Z","shell.execute_reply.started":"2024-09-25T12:56:44.930634Z","shell.execute_reply":"2024-09-25T12:56:57.849744Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Class distribution:\nclass\ne    3365\np    3134\nName: count, dtype: int64\n\nImbalance ratio (minority/majority): 0.93\nNo significant class imbalance detected.\nTraining Logistic Regression...\nLogistic Regression Accuracy: 0.93 Time: 0.04582333564758301\nTraining Decision Tree...\nDecision Tree Accuracy: 1.0 Time: 0.018677949905395508\nTraining Random Forest...\nRandom Forest Accuracy: 1.0 Time: 0.49588847160339355\nTraining SVM...\nSVM Accuracy: 1.0 Time: 0.11274194717407227\nTraining KNN...\nKNN Accuracy: 1.0 Time: 0.15916037559509277\nTraining Gradient Boosting...\nGradient Boosting Accuracy: 1.0 Time: 0.4706897735595703\nTraining XGBoost...\nXGBoost Accuracy: 1.0 Time: 0.1379077434539795\nTraining AdaBoost...\nAdaBoost Accuracy: 1.0 Time: 0.294081449508667\nTraining Naive Bayes...\nNaive Bayes Accuracy: 0.88 Time: 0.00832366943359375\nTraining MLP Neural Network...\nMLP Neural Network Accuracy: 1.0 Time: 4.089228868484497\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"import shap\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nimport time\nimport xgboost as xgb\n\n# Preprocessing function with SHAP\ndef preprocess_data_with_shap(df, target_column, shap_threshold=0.01):\n    # Handle missing values\n    df = df.dropna()\n    \n    # Encode categorical features\n    label_encoders = {}\n    for col in df.select_dtypes(include=['object']).columns:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n        label_encoders[col] = le\n        \n    # Split data into features and labels\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    # Train a base model (Random Forest)\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n    \n    # Calculate SHAP values\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_train)\n    \n    # Calculate mean absolute SHAP values for feature importance\n    shap_importance = np.mean(np.abs(shap_values[1]), axis=0)\n    feature_importance = pd.DataFrame({'feature': df.drop(target_column, axis=1).columns, \n                                       'importance': shap_importance})\n    \n    # Remove features with importance less than the threshold\n    important_features = feature_importance[feature_importance['importance'] > shap_threshold]['feature'].values\n    X_train = pd.DataFrame(X_train, columns=df.drop(target_column, axis=1).columns)[important_features]\n    X_test = pd.DataFrame(X_test, columns=df.drop(target_column, axis=1).columns)[important_features]\n    \n    return X_train, X_test, y_train, y_test\n\n# Function to evaluate models\ndef evaluate_model(X_train, X_test, y_train, y_test):\n    models = {\n        \"Logistic Regression\": LogisticRegression(),\n        \"Decision Tree\": DecisionTreeClassifier(),\n        \"Random Forest\": RandomForestClassifier(),\n        \"SVM\": SVC(),\n        \"KNN\": KNeighborsClassifier(),\n        \"Gradient Boosting\": GradientBoostingClassifier(),\n        \"XGBoost\": xgb.XGBClassifier(),\n        \"AdaBoost\": AdaBoostClassifier(),\n        \"Naive Bayes\": GaussianNB(),\n        \"MLP Neural Network\": MLPClassifier()\n    }\n    \n    for name, model in models.items():\n        start_time = time.time()\n        print(f\"Training {name}...\")\n        model = model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        time_consumed = time.time() - start_time\n        print(f\"{name} Accuracy: {np.round(accuracy,2)} Time: {time_consumed}\")\n\n# Run all classifiers\ndef run_all_classifiers(df, target_column):\n    X_train, X_test, y_train, y_test = preprocess_data_with_shap(df, target_column)\n    evaluate_model(X_train, X_test, y_train, y_test)\n\n# Example usage with a dataset\nif __name__ == \"__main__\":\n    # Load your dataset here\n    df = pd.read_csv(\"/kaggle/input/mushroom-classification/mushrooms.csv\")\n    target_column = 'class'  # Change this to your dataset's target column\n    \n    # Run all classifiers on the dataset\n    run_all_classifiers(df, target_column)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:59:43.563393Z","iopub.execute_input":"2024-09-25T12:59:43.563909Z","iopub.status.idle":"2024-09-25T12:59:43.577779Z","shell.execute_reply.started":"2024-09-25T12:59:43.563861Z","shell.execute_reply":"2024-09-25T12:59:43.576501Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'import shap\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.neural_network import MLPClassifier\\nimport time\\nimport xgboost as xgb\\n\\n# Preprocessing function with SHAP\\ndef preprocess_data_with_shap(df, target_column, shap_threshold=0.01):\\n    # Handle missing values\\n    df = df.dropna()\\n    \\n    # Encode categorical features\\n    label_encoders = {}\\n    for col in df.select_dtypes(include=[\\'object\\']).columns:\\n        le = LabelEncoder()\\n        df[col] = le.fit_transform(df[col])\\n        label_encoders[col] = le\\n        \\n    # Split data into features and labels\\n    X = df.drop(target_column, axis=1)\\n    y = df[target_column]\\n    \\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n    \\n    # Standardize features\\n    scaler = StandardScaler()\\n    X_train = scaler.fit_transform(X_train)\\n    X_test = scaler.transform(X_test)\\n    \\n    # Train a base model (Random Forest)\\n    model = RandomForestClassifier()\\n    model.fit(X_train, y_train)\\n    \\n    # Calculate SHAP values\\n    explainer = shap.TreeExplainer(model)\\n    shap_values = explainer.shap_values(X_train)\\n    \\n    # Calculate mean absolute SHAP values for feature importance\\n    shap_importance = np.mean(np.abs(shap_values[1]), axis=0)\\n    feature_importance = pd.DataFrame({\\'feature\\': df.drop(target_column, axis=1).columns, \\n                                       \\'importance\\': shap_importance})\\n    \\n    # Remove features with importance less than the threshold\\n    important_features = feature_importance[feature_importance[\\'importance\\'] > shap_threshold][\\'feature\\'].values\\n    X_train = pd.DataFrame(X_train, columns=df.drop(target_column, axis=1).columns)[important_features]\\n    X_test = pd.DataFrame(X_test, columns=df.drop(target_column, axis=1).columns)[important_features]\\n    \\n    return X_train, X_test, y_train, y_test\\n\\n# Function to evaluate models\\ndef evaluate_model(X_train, X_test, y_train, y_test):\\n    models = {\\n        \"Logistic Regression\": LogisticRegression(),\\n        \"Decision Tree\": DecisionTreeClassifier(),\\n        \"Random Forest\": RandomForestClassifier(),\\n        \"SVM\": SVC(),\\n        \"KNN\": KNeighborsClassifier(),\\n        \"Gradient Boosting\": GradientBoostingClassifier(),\\n        \"XGBoost\": xgb.XGBClassifier(),\\n        \"AdaBoost\": AdaBoostClassifier(),\\n        \"Naive Bayes\": GaussianNB(),\\n        \"MLP Neural Network\": MLPClassifier()\\n    }\\n    \\n    for name, model in models.items():\\n        start_time = time.time()\\n        print(f\"Training {name}...\")\\n        model = model.fit(X_train, y_train)\\n        y_pred = model.predict(X_test)\\n        accuracy = accuracy_score(y_test, y_pred)\\n        time_consumed = time.time() - start_time\\n        print(f\"{name} Accuracy: {np.round(accuracy,2)} Time: {time_consumed}\")\\n\\n# Run all classifiers\\ndef run_all_classifiers(df, target_column):\\n    X_train, X_test, y_train, y_test = preprocess_data_with_shap(df, target_column)\\n    evaluate_model(X_train, X_test, y_train, y_test)\\n\\n# Example usage with a dataset\\nif __name__ == \"__main__\":\\n    # Load your dataset here\\n    df = pd.read_csv(\"/kaggle/input/mushroom-classification/mushrooms.csv\")\\n    target_column = \\'class\\'  # Change this to your dataset\\'s target column\\n    \\n    # Run all classifiers on the dataset\\n    run_all_classifiers(df, target_column)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:59:50.536870Z","iopub.execute_input":"2024-09-25T12:59:50.537318Z","iopub.status.idle":"2024-09-25T12:59:52.086674Z","shell.execute_reply.started":"2024-09-25T12:59:50.537274Z","shell.execute_reply":"2024-09-25T12:59:52.085472Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Training Linear Regression...\nLinear Regression MSE: 1771751116594.04 Time: 0.015462398529052734\nTraining Decision Tree...\nDecision Tree MSE: 2835977158256.88 Time: 0.002724170684814453\nTraining Random Forest...\nRandom Forest MSE: 1926061032226.21 Time: 0.292468786239624\nTraining SVM...\nSVM MSE: 5567937467895.9 Time: 0.01466989517211914\nTraining KNN...\nKNN MSE: 2106456221466.06 Time: 0.0030815601348876953\nTraining Gradient Boosting...\nGradient Boosting MSE: 1699486795075.28 Time: 0.09216928482055664\nTraining XGBoost...\nXGBoost MSE: 2032404618961.44 Time: 0.06665825843811035\nTraining AdaBoost...\nAdaBoost MSE: 2214324085042.79 Time: 0.12534332275390625\nTraining MLP Neural Network...\nMLP Neural Network MSE: 30127923406265.48 Time: 0.8832221031188965\n","output_type":"stream"},{"name":"stderr","text":"Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}